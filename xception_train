%pip install pandas
%pip install matplotlib
%pip install numpy
%pip install scipy
%pip install scikit-learn
%pip install kagglehub
%pip install tensorflow
import tensorflow.keras
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
%pip install "roboflow[desktop]"
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
import json
import matplotlib.image as mpimg
from PIL import Image
from tensorflow.keras.preprocessing.image import load_img, img_to_array
#______________________________________________________________________________________________________________________________
model = load_model('path/to/your/model') #–º–æ–¥–µ–ª—å xception
base_dir = 'path/to/your/dataset' #–¥–∞—Ç–∞—Å–µ—Ç https://www.kaggle.com/datasets/manjilkarki/deepfake-and-real-images/data
#______________________________________________________________________________________________________________________________
# 1. –í–∫–ª—é—á–∞–µ–º eager execution
import tensorflow as tf
tf.config.run_functions_eagerly(True)

# 2. –û–ü–†–ï–î–ï–õ–Ø–ï–ú –ü–ï–†–ï–ú–ï–ù–ù–´–ï
img_size = 224      # —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
batch_size = 32     # –Ø–í–ù–û –æ–ø—Ä–µ–¥–µ–ª—è–µ–º batch_size

# 3. –°–û–ó–î–ê–ï–ú –ì–ï–ù–ï–†–ê–¢–û–†–´ –î–ê–ù–ù–´–• –ü–†–ï–ñ–î–ï –í–°–ï–ì–û
print("="*60)
print("–°–û–ó–î–ê–ï–ú –ì–ï–ù–ï–†–ê–¢–û–†–´ –î–ê–ù–ù–´–•")
print("="*60)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö 80/20
train_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2  # 20% –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
)

valid_datagen = ImageDataGenerator(rescale=1./255)

# –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
train_gen = train_datagen.flow_from_directory(
    base_dir,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training',  # 80% –¥–∞–Ω–Ω—ã—Ö
    shuffle=True
)

valid_gen = train_datagen.flow_from_directory(
    base_dir,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation',  # 20% –¥–∞–Ω–Ω—ã—Ö
    shuffle=False
)

print(f"‚úì Train images: {train_gen.samples}")
print(f"‚úì Valid images: {valid_gen.samples}")
print(f"‚úì –ö–ª–∞—Å—Å–æ–≤ –≤ train_gen: {train_gen.num_classes}")
print(f"‚úì –ö–ª–∞—Å—Å–æ–≤ –≤ valid_gen: {valid_gen.num_classes}")
print(f"‚úì Batch size: {batch_size}")

# 4. –¢–ï–ü–ï–†–¨ –ò–°–ü–†–ê–í–õ–Ø–ï–ú –ú–û–î–ï–õ–¨
print("\n" + "="*60)
print("–ò–°–ü–†–ê–í–õ–Ø–ï–ú –í–ê–®–£ –ú–û–î–ï–õ–¨")
print("="*60)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –í–ê–®–ï–ô –º–æ–¥–µ–ª–∏
print("1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞—à—É –º–æ–¥–µ–ª—å...")
model_config = model.get_config()

# –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
print("2. –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π...")

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –ò–ó –ì–ï–ù–ï–†–ê–¢–û–†–ê
num_classes = train_gen.num_classes
print(f"3. –û–ø—Ä–µ–¥–µ–ª–∏–ª–∏ {num_classes} –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö")

# –î–ª—è Sequential –º–æ–¥–µ–ª–µ–π:
if isinstance(model, tf.keras.Sequential):
    print("–ú–æ–¥–µ–ª—å Sequential...")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Å —Ç–µ–º–∏ –∂–µ —Å–ª–æ—è–º–∏, –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
    new_model = tf.keras.Sequential()
    for layer in model.layers[:-1]:  # –≤—Å–µ —Å–ª–æ–∏ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
        new_model.add(layer.__class__.from_config(layer.get_config()))
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    print(f"4. –î–æ–±–∞–≤–ª—è–µ–º –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π –¥–ª—è {num_classes} –∫–ª–∞—Å—Å–æ–≤...")
    
    if num_classes == 2:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
        x_sample, y_sample = next(train_gen)
        print(f"   –§–æ—Ä–º–∞—Ç y_sample: {y_sample.shape}")
        
        if y_sample.shape[1] == 1:
            new_model.add(Dense(1, activation='sigmoid'))
            loss = 'binary_crossentropy'
            print("   –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–Ω–∞—Ä–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é (sigmoid)")
        else:
            new_model.add(Dense(2, activation='softmax'))
            loss = 'categorical_crossentropy'
            print("   –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è 2 –∫–ª–∞—Å—Å–æ–≤")
    else:
        new_model.add(Dense(num_classes, activation='softmax'))
        loss = 'categorical_crossentropy'
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è {num_classes} –∫–ª–∞—Å—Å–æ–≤")
    
    # –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞ –ò–ó –í–ê–®–ï–ô –ú–û–î–ï–õ–ò
    print("5. –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞ –∏–∑ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏...")
    for i in range(len(model.layers) - 1):
        try:
            new_model.layers[i].set_weights(model.layers[i].get_weights())
            print(f"   ‚úì –í–µ—Å–∞ —Å–ª–æ—è {i} —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã")
        except:
            print(f"   ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞ —Å–ª–æ—è {i}")

# –î–ª—è Functional API –º–æ–¥–µ–ª–µ–π:
else:
    print("–ú–æ–¥–µ–ª—å Functional API...")
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—ã—Ö–æ–¥ –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–ª–æ—è
    x = model.layers[-2].output
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã—Ö–æ–¥
    if num_classes == 2:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
        x_sample, y_sample = next(train_gen)
        print(f"   –§–æ—Ä–º–∞—Ç y_sample: {y_sample.shape}")
        
        if y_sample.shape[1] == 1:
            output = Dense(1, activation='sigmoid')(x)
            loss = 'binary_crossentropy'
        else:
            output = Dense(2, activation='softmax')(x)
            loss = 'categorical_crossentropy'
    else:
        output = Dense(num_classes, activation='softmax')(x)
        loss = 'categorical_crossentropy'
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å
    new_model = Model(inputs=model.input, outputs=output)
    
    # –ö–æ–ø–∏—Ä—É–µ–º –≤–µ—Å–∞
    for i in range(len(model.layers) - 1):
        try:
            new_model.layers[i].set_weights(model.layers[i].get_weights())
            print(f"   ‚úì –í–µ—Å–∞ —Å–ª–æ—è {i} —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω—ã")
        except:
            print(f"   ‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞ —Å–ª–æ—è {i}")

# 5. –°–û–ó–î–ê–ï–ú –ù–û–í–´–ô –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†
print("\n6. –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä...")

# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Ç–∏–ø –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, —á—Ç–æ –∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
if hasattr(model, 'optimizer'):
    optimizer_name = model.optimizer.__class__.__name__
    print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {optimizer_name}")
    
    if optimizer_name == 'Adam':
        from tensorflow.keras.optimizers import Adam
        optimizer = Adam(learning_rate=0.001)
    elif optimizer_name == 'SGD':
        from tensorflow.keras.optimizers import SGD
        optimizer = SGD(learning_rate=0.01)
    elif optimizer_name == 'RMSprop':
        from tensorflow.keras.optimizers import RMSprop
        optimizer = RMSprop(learning_rate=0.001)
    else:
        optimizer = 'adam'
else:
    optimizer = 'adam'

print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: {optimizer}")

# 6. –ö–û–ú–ü–ò–õ–ò–†–£–ï–ú –° –ù–û–í–´–ú –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†–û–ú
print("\n7. –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å...")
new_model.compile(
    optimizer=optimizer,
    loss=loss,
    metrics=['accuracy']
)

print("‚úì –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –∏ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞!")
print("\n–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏:")
new_model.summary()

# 7. –ü–†–û–í–ï–†–Ø–ï–ú –ò –û–ë–£–ß–ê–ï–ú
print("\n" + "="*60)
print("–ü–†–û–í–ï–†–ö–ê –ò –û–ë–£–ß–ï–ù–ò–ï")
print("="*60)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
try:
    x_test, y_test = next(train_gen)
    print(f"–¢–µ—Å—Ç–æ–≤—ã–π –±–∞—Ç—á: x.shape={x_test.shape}, y.shape={y_test.shape}")
    
    predictions = new_model.predict(x_test[:2], verbose=0)
    print(f"Predict –≤—ã–ø–æ–ª–Ω–µ–Ω! –í—ã—Ö–æ–¥ shape: {predictions.shape}")
    
    if predictions.shape[1] == num_classes or (num_classes == 1 and predictions.shape[1] == 1):
        print("‚úì –ú–û–î–ï–õ–¨ –ì–û–¢–û–í–ê –ö –û–ë–£–ß–ï–ù–ò–Æ!")
    else:
        print(f"‚ö† –í–Ω–∏–º–∞–Ω–∏–µ: –æ–∂–∏–¥–∞–ª–æ—Å—å (2, {num_classes}), –ø–æ–ª—É—á–µ–Ω–æ {predictions.shape}")
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")

# 8. –ü–†–ê–í–ò–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢ –®–ê–ì–û–í –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø
print("\n" + "="*60)
print("–†–ê–°–ß–ï–¢ –®–ê–ì–û–í –î–õ–Ø –û–ë–£–ß–ï–ù–ò–Ø")
print("="*60)

# –ü–†–ê–í–ò–õ–¨–ù–´–ô —Ä–∞—Å—á–µ—Ç —à–∞–≥–æ–≤:
# steps_per_epoch = –æ–±—â–µ–µ_–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π_–¥–ª—è_—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ / batch_size

# –í–∞—Ä–∏–∞–Ω—Ç 1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
steps_per_epoch = len(train_gen)  # train_gen —É–∂–µ –∑–Ω–∞–µ—Ç —Å–∫–æ–ª—å–∫–æ —É –Ω–µ–≥–æ –±–∞—Ç—á–µ–π
validation_steps = len(valid_gen)

print(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç:")
print(f"  Steps per epoch: {steps_per_epoch}")
print(f"  Validation steps: {validation_steps}")

# –í–∞—Ä–∏–∞–Ω—Ç 2: –†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
manual_train_steps = train_gen.samples // batch_size
if train_gen.samples % batch_size != 0:
    manual_train_steps += 1  # –¥–æ–±–∞–≤–ª—è–µ–º –Ω–µ–ø–æ–ª–Ω—ã–π –±–∞—Ç—á
    
manual_valid_steps = valid_gen.samples // batch_size
if valid_gen.samples % batch_size != 0:
    manual_valid_steps += 1

print(f"\n–†—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏):")
print(f"  Train images: {train_gen.samples}")
print(f"  Valid images: {valid_gen.samples}")
print(f"  Train steps: {manual_train_steps}")
print(f"  Valid steps: {manual_valid_steps}")

print(f"\n–ò—Å–ø–æ–ª—å–∑—É–µ–º: {steps_per_epoch} —à–∞–≥–æ–≤ –Ω–∞ —ç–ø–æ—Ö—É")
print(f"          {validation_steps} —à–∞–≥–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏")

# 9. –ó–ê–ü–£–°–ö–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï
print("\n" + "="*60)
print("–ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï...")
print("="*60)

# –í–∞—Ä–∏–∞–Ω—Ç A: –ë–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è —à–∞–≥–æ–≤ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
history = new_model.fit(
    train_gen,  # –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–∞–º –∑–Ω–∞–µ—Ç —Å–∫–æ–ª—å–∫–æ —É –Ω–µ–≥–æ —à–∞–≥–æ–≤
    validation_data=valid_gen,
    epochs=3,
    verbose=1
)

# –ò–õ–ò –í–∞—Ä–∏–∞–Ω—Ç B: –° —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —à–∞–≥–æ–≤
# history = new_model.fit(
#     train_gen,
#     steps_per_epoch=steps_per_epoch,
#     validation_data=valid_gen,
#     validation_steps=validation_steps,
#     epochs=3,
#     verbose=1
# )

print("\n" + "="*60)
print("‚úì ‚úì ‚úì –£–°–ü–ï–•! –ú–û–î–ï–õ–¨ –û–ë–£–ß–ï–ù–ê!")
print("="*60)
print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {history.history['accuracy'][-1]:.4f}")
print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {history.history['val_accuracy'][-1]:.4f}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
new_model.save('path/to/your/new/model')
print("\n‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'your_saved_million.h5'")
print("‚úì –í–∞—à –º–∏–ª–ª–∏–æ–Ω —Å–ø–∞—Å–µ–Ω! üéâ")
#______________________________________________________________________________________________________________________________
